{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For extracting files from Amazon S3 Buckets\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "# For PySpark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# For Delta Lake\n",
    "from delta import *\n",
    "\n",
    "# Working with images\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# To speed up, track time\n",
    "import multiprocessing\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "# to dump data if something happens\n",
    "import pickle\n",
    "\n",
    "#to group flight ids and get counts\n",
    "from itertools import groupby\n",
    "\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-3.0.1-bin-hadoop2.7\"\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-03 23:24:21--  https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11948376 (11M) [application/java-archive]\n",
      "Saving to: ‘/opt/spark-3.0.1-bin-hadoop2.7/jars/aws-java-sdk-1.7.4.jar’\n",
      "\n",
      "aws-java-sdk-1.7.4. 100%[===================>]  11.39M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-05-03 23:24:21 (108 MB/s) - ‘/opt/spark-3.0.1-bin-hadoop2.7/jars/aws-java-sdk-1.7.4.jar’ saved [11948376/11948376]\n",
      "\n",
      "--2022-05-03 23:24:22--  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 126287 (123K) [application/java-archive]\n",
      "Saving to: ‘/opt/spark-3.0.1-bin-hadoop2.7/jars/hadoop-aws-2.7.3.jar’\n",
      "\n",
      "hadoop-aws-2.7.3.ja 100%[===================>] 123.33K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2022-05-03 23:24:22 (5.31 MB/s) - ‘/opt/spark-3.0.1-bin-hadoop2.7/jars/hadoop-aws-2.7.3.jar’ saved [126287/126287]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# These 2 links include the jar files needed to interact with AWS S3\n",
    "!wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar -P $SPARK_HOME/jars/\n",
    "!wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar -P $SPARK_HOME/jars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ede0ef552fa6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>drones</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff56859bef0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Spark Session\n",
    "spark = SparkSession.builder.appName(\"drones\") \\\n",
    "    .config(\"spark.executor.memory\", \"25g\") \\\n",
    "    .config(\"spark.driver.memory\", \"25g\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "schema = StructType([StructField(\"img_path\", StringType()),\n",
    "                    StructField(\"img_content\", StringType())])\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../img_path.pickle','rb') as file:\n",
    "    img_path = pickle.load(file)\n",
    "len(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_tenth = len(img_path)//10\n",
    "tenth1 = img_path[:one_tenth]\n",
    "tenth2 = img_path[one_tenth:2*one_tenth]\n",
    "tenth3 = img_path[2*one_tenth:3*one_tenth]\n",
    "tenth4 = img_path[3*one_tenth:4*one_tenth]\n",
    "tenth5 = img_path[4*one_tenth:5*one_tenth]\n",
    "tenth6 = img_path[5*one_tenth:6*one_tenth]\n",
    "tenth7 = img_path[6*one_tenth:7*one_tenth]\n",
    "tenth8 = img_path[7*one_tenth:8*one_tenth]\n",
    "tenth9 = img_path[8*one_tenth:9*one_tenth]\n",
    "last_tenth = img_path[9*one_tenth:]\n",
    "len(tenth1 + tenth2 + tenth3 + tenth4 + tenth5 + tenth6 + tenth7 + tenth8 + tenth9 + last_tenth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = multiprocessing.cpu_count()\n",
    "cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3', config=Config(signature_version=UNSIGNED))\n",
    "bucket = s3.Bucket(\"airborne-obj-detection-challenge-training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(image_name):\n",
    "    img_s3 = bucket.Object(image_name)\n",
    "    img_content = img_s3.get()['Body'].read()\n",
    "    img_PIL = Image.open(io.BytesIO(img_content))\n",
    "    img_smaller = img_PIL.convert('RGB').resize((224,224))\n",
    "    temp_img = io.BytesIO()\n",
    "    img_smaller.save(temp_img, format = \"png\")\n",
    "    png_encoded = base64.b64encode(temp_img.getvalue())\n",
    "    \n",
    "    return str(png_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_time = time.time()\n",
    "\n",
    "with multiprocessing.Pool(cpus) as p:\n",
    "    img_content = list(tqdm.tqdm(p.imap(download_images, tenth3), total = len(tenth3))) #switch name\n",
    "\n",
    "print(\"Multiprocessing time for Part 1.2 with\", cpus,\" Cores:\", time.time()-starting_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy as a list\n",
    "img_content2 = img_content\n",
    "len(img_content2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy as a pickle file\n",
    "with open('../tenth1.0.pickle','wb') as file:\n",
    "    pickle.dump(img_content,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in pickle file if kernel dies and need img content (without redownloading content)\n",
    "#with open('../tenth1.0.pickle','rb') as file:\n",
    " #   img_content = pickle.load(file)\n",
    "#len(img_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of flight ids (folder names)\n",
    "flight_ids = []\n",
    "for path_name in tenth3: #switch name\n",
    "    flight_ids.append(path_name[13:45])\n",
    "len(flight_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_img_counts = [len(list(group)) for key, group in groupby(flight_ids)]\n",
    "print(flight_img_counts[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flight_img_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "starting_time = time.time()\n",
    "\n",
    "for i in range(len(flight_img_counts)):\n",
    "    starting_time2 = time.time()\n",
    "    i = i + 1\n",
    "    prv_n_images = sum(flight_img_counts[0:i])\n",
    "    n_images = flight_img_counts[i]\n",
    "    flight_ids1 = flight_ids[prv_n_images:prv_n_images+n_images]\n",
    "    img_content1 = img_content[prv_n_images:prv_n_images+n_images]\n",
    "    flight_id = flight_ids1[0]\n",
    "\n",
    "    print(\"Starting upload for flight\", i)\n",
    "    img_path_rdd = sc.parallelize(flight_ids1)\n",
    "    img_content_rdd = sc.parallelize(img_content1)\n",
    "    zipped_rdd = img_path_rdd.zip(img_content_rdd).collect()\n",
    "    df = spark.createDataFrame(zipped_rdd, schema)\n",
    "    \n",
    "    s3_bucket_name = \"s3a://drones-project-test/\" + flight_id + \"/\"\n",
    "    df.write.format(\"delta\").mode(\"append\").save(s3_bucket_name)\n",
    "    print(\"Total time to upload flight\", i, \"(#\", flight_id, \") to our own s3 bucket took\", time.time()-starting_time2, \"seconds.\")\n",
    "\n",
    "    del img_content1\n",
    "    del img_path_rdd\n",
    "    del img_content_rdd\n",
    "    del zipped_rdd\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Upload time for 131 (1 tenth of the) flights with\", cpus,\"cpus:\", time.time()-starting_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
